# üõ†Ô∏è Laboratorio: Creaci√≥n y exploraci√≥n de dashboards con datos reales en Kibana

## üìã Objetivo

Crear dashboards interactivos en Kibana con datos reales almacenados previamente en Elasticsearch, utilizando una VM en Azure creada previamente.

---

## üìù Requisitos previos

* Haber completado el [Laboratorio D√≠a 1](#) (Elasticsearch en Azure).
* VM con Elasticsearch instalado (Ubuntu 22.04).
* Acceso SSH y al Azure Portal.

---

## ‚öôÔ∏è Paso 1: Instalaci√≥n de Kibana en Azure VM

**Con√©ctate v√≠a SSH a tu VM Azure:**

```bash
ssh usuario@<ip-publica-vm>
```

**Actualiza repositorios e instala Kibana:**

```bash
sudo apt update
sudo apt install kibana -y
```

---

## ‚öôÔ∏è Paso 2: Configuraci√≥n b√°sica de Kibana

Edita el archivo de configuraci√≥n Kibana:

```bash
sudo nano /etc/kibana/kibana.yml
```

Agrega o modifica la siguiente l√≠nea para conectar Kibana con Elasticsearch:

```yaml
elasticsearch.hosts: ["http://localhost:9200"]
server.host: "0.0.0.0"
```

Guarda el archivo (`Ctrl+X`, `Y`, `Enter`).

**Inicia y habilita Kibana:**

```bash
sudo systemctl enable kibana
sudo systemctl start kibana
sudo systemctl status kibana
```

---

## ‚öôÔ∏è Paso 3: Permitir acceso externo a Kibana desde Azure

* En Azure Portal, accede a tu VM ‚Üí Networking.
* Agrega una regla entrante para permitir acceso al puerto TCP `5601`.

**Verifica en navegador:**

```
http://<ip-publica-vm>:5601
```

---

## ‚öôÔ∏è Paso 4: Crear √≠ndice en Kibana

* Accede a Kibana desde tu navegador.
* Ve a **Stack Management ‚Üí Data ‚Üí Index Patterns**.
* Click en **Create Index Pattern**.
* Escribe el nombre del √≠ndice creado el d√≠a anterior (ej. `mi_indice`) y haz clic en **Next Step**.
* Selecciona el campo de tiempo si aplica, o **skip** si no lo tienes.
* Finaliza con **Create Index Pattern**.


OPCI√ìN A: Agregar informaci√≥n:

1. Verifica si tu √≠ndice tiene datos en Elasticsearch
En tu servidor o desde una terminal con acceso a Elasticsearch:

curl -X GET "localhost:9200/mi_indice/_search?pretty"

O (para solo contar documentos):

curl -X GET "localhost:9200/mi_indice/_count?pretty"

Si el resultado es {"count":0...}, tu √≠ndice est√° vac√≠o.

Si hay datos pero no los ves en Kibana, revisa el rango de fechas (ver punto 3).

2. Ingresa datos de ejemplo al √≠ndice (si est√° vac√≠o)
Puedes insertar documentos manualmente para pruebas r√°pidas:

curl -X POST "localhost:9200/mi_indice/_doc" -H 'Content-Type: application/json' -d'
{
  "curso": "ELK B√°sico",
  "empresa": "Kyndryl",
  "duracion": 6,
  "fecha": "2024-05-19T10:00:00Z",
  "status": "200"
}'

Repite con m√°s ejemplos cambiando los valores.

Vuelve a consultar con /_count y aseg√∫rate que tienes varios documentos.

3. Ajusta el rango de fechas en Kibana

MUY IMPORTANTE:
Kibana, por defecto, solo muestra datos recientes (√∫ltimos 15 minutos/√∫ltima hora).

En la parte superior derecha, cambia el rango de tiempo a un rango mayor (por ejemplo: Last 7 days, This year, o un rango absoluto que cubra las 
fechas de tus documentos de prueba).

Si tus documentos no tienen campo de tiempo compatible (@timestamp), crea el √≠ndice sin campo temporal o selecciona ‚ÄúI don‚Äôt want to use the time filter‚Äù.

4. Actualiza y vuelve a crear las visualizaciones
Cuando confirmes que tienes datos:

Ve a Discover y verifica que aparecen documentos.

Ahora, en Dashboards, selecciona los campos deseados en la visualizaci√≥n.

Si solo ves campos ‚ÄúEmpty fields‚Äù, es porque no hay datos indexados o el mapping est√° mal.

5. (Opcional) Reindexa o ingresa nuevos datos si cambiaste el mapping

Si cambiaste el mapping o el formato del √≠ndice, puede que debas reindexar o borrar/recrear el √≠ndice.

Resumen de pasos r√°pidos:

Verifica si tienes datos en el √≠ndice:

curl -X GET "localhost:9200/mi_indice/_count?pretty"

Inserta documentos de prueba si est√° vac√≠o.

Aseg√∫rate de que el rango de fechas en Kibana cubra los documentos.

Confirma que ves datos en Discover antes de crear dashboards.

Vuelve a construir visualizaciones con los nuevos campos que aparecen.



OPCI√ìN B: Agregar informaci√≥n:

1. M√©todo r√°pido: Usando cURL + script Bash (para peque√±os datasets JSON)
Si tienes un archivo .json (por ejemplo, una lista de documentos) puedes hacer un script para cargarlo en lote:

Sup√≥n que tienes un archivo llamado datos.jsonl donde cada l√≠nea es un documento JSON.

while read line; do
  curl -X POST "localhost:9200/mi_indice/_doc" -H 'Content-Type: application/json' -d "$line"
done < datos.jsonl

Ventaja: Simple para pocos miles de registros.

Limite: No recomendado para datasets de cientos de miles o millones.

2. M√©todo profesional: Bulk API de Elasticsearch
La Bulk API es la mejor forma para insertar masivamente datos en Elasticsearch.

A. Prepara tu dataset en formato ‚ÄúNDJSON‚Äù
(Ejemplo, cada l√≠nea: acci√≥n y documento)

{ "index": {} }
{ "curso": "ELK B√°sico", "empresa": "Kyndryl", "duracion": 6, "fecha": "2024-05-19T10:00:00Z", "status": "200" }
{ "index": {} }
{ "curso": "Kubernetes", "empresa": "Microsoft", "duracion": 8, "fecha": "2024-05-18T11:00:00Z", "status": "404" }
...

Gu√°rdalo como bulk_datos.json.

B. Carga el dataset con un solo comando:

curl -X POST "localhost:9200/mi_indice/_bulk" -H 'Content-Type: application/x-ndjson' --data-binary @bulk_datos.json
Ventaja: S√∫per eficiente, soporta decenas de miles de documentos en una sola llamada.

Dataset p√∫blico: Puedes usar cualquier dataset en CSV/JSON y transformarlo a NDJSON con Python, jq o herramientas similares.

3. Usando Logstash (ideal para CSV, JSON, o ingesti√≥n continua)
Logstash permite cargar datos masivos, transformar y enviar directo a Elasticsearch.

Ejemplo de archivo de configuraci√≥n para CSV:

logstash.conf

conf
Copy
Edit
input {
  file {
    path => "/ruta/al/dataset.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}
filter {
  csv {
    separator => ","
    columns => ["campo1","campo2","campo3","fecha","status"]
  }
  date {
    match => ["fecha", "yyyy-MM-dd'T'HH:mm:ss'Z'"]
    target => "@timestamp"
  }
}
output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "mi_indice"
  }
  stdout { codec => rubydebug }
}

Ventaja: Excelente para transformar, enriquecer y cargar datos estructurados de cualquier tama√±o.

Ejecuta Logstash:

logstash -f logstash.conf

4. Dataset p√∫blicos recomendados

Kaggle Datasets

Datos Abiertos del Gobierno

UCI Machine Learning Repository

Descarga en CSV o JSON, usa uno de los m√©todos anteriores para cargar a Elasticsearch.

5. Tips extra
Si tienes un CSV, puedes convertirlo a JSON o NDJSON con Python:

import csv, json
with open('datos.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        print(json.dumps(row))
        
Si tu dataset es muy grande, divide en partes (archivos m√°s peque√±os) y usa bulk por lotes.

OPCI√ìN C: Agregar informaci√≥n:

Ejemplo: Cargar datos de ventas (CSV) a Elasticsearch usando Logstash
1. Descarga y revisa el dataset
Descarga el archivo de Kaggle, por ejemplo: OnlineRetail.csv

Col√≥calo en tu VM o servidor donde tengas Logstash.

2. Crea el √≠ndice en Elasticsearch (opcional, para definir mapping)
Ejemplo b√°sico de mapping (aj√∫stalo seg√∫n tus campos):

curl -X PUT "localhost:9200/ventas" -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "InvoiceNo":   { "type": "keyword" },
      "StockCode":   { "type": "keyword" },
      "Description": { "type": "text" },
      "Quantity":    { "type": "integer" },
      "InvoiceDate": { "type": "date", "format": "MM/dd/yyyy HH:mm" },
      "UnitPrice":   { "type": "float" },
      "CustomerID":  { "type": "keyword" },
      "Country":     { "type": "keyword" }
    }
  }
}'

3. Crea el archivo de configuraci√≥n para Logstash
Guarda esto como ventas.conf:

input {
  file {
    path => "/ruta/OnlineRetail.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}
filter {
  csv {
    separator => ","
    skip_header => "true"
    columns => ["InvoiceNo","StockCode","Description","Quantity","InvoiceDate","UnitPrice","CustomerID","Country"]
  }
  mutate {
    convert => { "Quantity" => "integer" }
    convert => { "UnitPrice" => "float" }
  }
  date {
    match => ["InvoiceDate", "MM/dd/yyyy HH:mm"]
    target => "@timestamp"
  }
}
output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "ventas"
  }
  stdout { codec => rubydebug }
}


Ajusta la ruta del archivo CSV seg√∫n la ubicaci√≥n real en tu VM.

4. Ejecuta Logstash para importar el dataset

logstash -f ventas.conf
Esto leer√° el CSV, transformar√° los datos, y los cargar√° en el √≠ndice ventas de Elasticsearch.

5. Visualiza y analiza los datos en Kibana
Crea un Data View en Kibana llamado ventas.

Explora los campos: podr√°s filtrar por pa√≠s, ver tendencias de ventas por fecha, detectar picos en la cantidad de ventas, etc.

Prueba crear dashboards:

Total de ventas por pa√≠s (Pie chart)

Evoluci√≥n de ventas por d√≠a (Line chart)

Productos m√°s vendidos (Bar chart)

An√°lisis de clientes (Tablas y gr√°ficos combinados)

¬øNo tienes Logstash? M√©todo alternativo con Bulk API
Convierte el CSV a NDJSON con Python o cualquier conversor, y usa el m√©todo /_bulk explicado antes.

---

## ‚öôÔ∏è Paso 5: Crear visualizaciones b√°sicas

Ve a **Analytics ‚Üí Dashboard ‚Üí Create new**.

* Click **Create Visualization**.
* Selecciona **Bar Chart**:

  * Configura en eje X el campo deseado (ej.: `status`) y Y para contar documentos.
* A√±ade visualizaci√≥n.

Repite con otras visualizaciones:

* **Pie Chart**: proporci√≥n de documentos por campo (ej. `empresa`).
* **Line Chart**: tendencias en cantidad de documentos en el tiempo.
* Guarda cada visualizaci√≥n individualmente.

---

## ‚öôÔ∏è Paso 6: Creaci√≥n de un Dashboard

* En **Dashboard**, haz click en **Add from library**.
* Selecciona visualizaciones creadas anteriormente.
* Organiza visualizaciones en el dashboard usando drag & drop.
* Guarda dashboard con nombre relevante (ej. `Dashboard_Curso`).

---

## ‚öôÔ∏è Paso 7: Uso b√°sico de Kibana Query Language (KQL)

Desde tu dashboard:

Agregar datos para los KQL:

curl -X POST "localhost:9200/mi_indice/_doc" -H 'Content-Type: application/json' -d'
{
  "curso": "ELK B√°sico",
  "empresa": "Kyndryl",
  "duracion": 12,
  "fecha": "2024-05-19T10:00:00Z",
  "status": "200",
  "bytes": 1100
}'

curl -X POST "localhost:9200/mi_indice/_doc" -H 'Content-Type: application/json' -d'
{
  "curso": "DevOps",
  "empresa": "Microsoft",
  "duracion": 8,
  "fecha": "2024-05-18T11:00:00Z",
  "status": "404",
  "bytes": 800
}'

curl -X POST "localhost:9200/mi_indice/_doc" -H 'Content-Type: application/json' -d'
{
  "curso": "Azure Monitor",
  "empresa": "Kyndryl",
  "duracion": 10,
  "fecha": "2024-05-17T09:00:00Z",
  "status": "200",
  "bytes": 600
}'

curl -X POST "localhost:9200/mi_indice/_doc" -H 'Content-Type: application/json' -d'
{
  "curso": "Kubernetes",
  "empresa": "Google",
  "duracion": 16,
  "fecha": "2024-05-16T13:00:00Z",
  "status": "200",
  "bytes": 1500
}'

curl -X POST "localhost:9200/mi_indice/_doc" -H 'Content-Type: application/json' -d'
{
  "curso": "Observabilidad",
  "empresa": "Kyndryl",
  "duracion": 14,
  "fecha": "2024-05-15T15:00:00Z",
  "status": "500",
  "bytes": 1300
}'

* Usa la barra superior para filtrar usando consultas b√°sicas KQL:

Ejemplos:

* Filtrar por un valor espec√≠fico:

```kql
empresa: "Kyndryl"
```

* Rango num√©rico:

```kql
bytes > 1000
```

* Combinar condiciones:

```kql
status: "200" AND bytes > 500
```

Observa c√≥mo cambian las visualizaciones din√°micamente.

---

## ‚öôÔ∏è Paso 8: Compartir y exportar Dashboards

* Guarda los dashboards creados.
* Explora opciones para compartir dashboard (exportar PDF, generar enlaces).

---

## üöÆ Limpieza (Opcional)

Si ya no usar√°s estos recursos, elimina desde Azure Portal:

* Ve a tu grupo de recursos y elimina recursos para evitar costos innecesarios.

---

## üìå Recomendaciones finales

* Documenta y guarda evidencia de visualizaciones creadas.
* Comparte cualquier duda o consulta durante la sesi√≥n.

---

¬°Excelente trabajo! üöÄ Ahora has creado dashboards interactivos en Kibana conectados a Elasticsearch, fortaleciendo tus habilidades en an√°lisis y visualizaci√≥n de datos.
